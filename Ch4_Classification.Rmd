---
title: "Ch4_Classiication_Problems"
author: "G Smith"
date: "October 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

## 4 Classification

The response value is qualitative and takes discrete values in an "unordered" set
```{r}
# Loading my standard work environment libraries and the ISLR library
library(MASS) # LDA package
library(class) # K-Nearest Neighbors package
library(broom)
library(gridExtra)
library(GGally)
library(knitr)
library(ISLR)
library(car)
library(caret)
library(tidyverse)
# note that dplyr select() is masked by MASS library. Need to use dplyr::select()
```

### 4.1 Introduction to Classification Problems
```{r}
# loading the "Default" data set used in the section
data(Default)
Default <- as.tibble(Default)
Default
```
```{r}
# plotting Balance vs Income and Default vs Balance
Default %>% 
  sample_n(1500, replace = FALSE) %>%   # pulling a sample of 1500 for the plot
  ggplot(aes(balance, income, color = default, shape = default)) +
  geom_point()
```
Obseravtions: there are many fewer "Yes" compared to "No" - the prior probablity of a "Yes" is much lower than a "No"
higher balances seem to have more "Yes" defaults. Doesn't appear to be a default pattern based on income
```{r}
# box plots of dafault vs balance and income
# putting boxplots side by side
p1 <-  Default %>% 
  ggplot(aes(x = default, y = balance, color = default)) +
  geom_boxplot()
p2 <- Default %>% 
  ggplot(aes(x = default, y = income, color = default)) +
  geom_boxplot()
grid.arrange(p1, p2, nrow = 1)
```
Observations: there appears a strong pattern between default status and balance; not so for default status and income. 
### 4.3 Logistic Regression - Used where there are 2 Response Classes 
```{r}
# building my own logistic function and plotting it
b0 <- -3.5
b1 <- 0.1
x <- seq(1:100)
p <- exp(b0 + b1 * x)/(1+exp(b0 + b1 * x))
my_log <- tibble(x = x,p = p)
my_log
my_log %>% 
ggplot(aes(x,p)) +
  geom_line()

```
```{r}
# plotting default where the predictor is qualitative - in this case student status
Default %>% 
  ggplot(aes(x = default, y = student, color = student)) +
  geom_count(aes(size = stat(prop), group = student)) +
  scale_size_area(max_size = 10)
```
4.3.1 The Logistic Model
```{r}
# logistic regression examples from the text book
# logistic regression w/ a continous predictor
log.fit.1 <- Default %>% 
  glm(default ~ balance, data = ., family = "binomial")
tidy(log.fit.1)
```
```{r}
summary(log.fit.1)
```
### 4.3.3
```{r}
# logistic regression w/ a discrete predictor
log.fit.2 <- Default %>% 
  glm(default ~ student, family = "binomial", data = .)
tidy(log.fit.2)
```
```{r}
summary(log.fit.2)
```
### 4.3.4 Multiple Logistic Regression
```{r}
log.fit.3 <- Default %>% 
  glm(default ~ balance + income + student, family = "binomial", data = .)
tidy(log.fit.3)
```
```{r}
summary(log.fit.3)
```
```{r}
# plots of student status vs income and balance
p1 <- Default %>% 
      ggplot(aes(student, income, color = student)) +
      geom_boxplot()
p2 <- Default %>% 
      ggplot(aes(student, balance, color = student)) +
      geom_boxplot()
grid.arrange(p1,p2, nrow = 1)
```
```{r}
p1 <- Default %>%
  filter(student == "Yes") %>% 
  ggplot(aes(balance,income, color = default, shape = student)) +
  geom_point()
p2 <- Default %>%
  filter(student == "No") %>% 
  ggplot(aes(balance,income, color = default, shape = student)) +
  geom_point()
grid.arrange(p1,p2,ncol = 1)
```
```{r}
p1 <- Default %>%
  filter(student == "Yes") %>% 
  ggplot(aes(balance)) +
  geom_density() +
  facet_grid(default ~.)
p2 <- Default %>%
  filter(student == "No") %>% 
  ggplot(aes(balance)) +
  geom_density() + 
  facet_grid(default ~.)
grid.arrange(p1,p2,ncol = 1)
```
### 4.4 Linear Discriminant Analysis (LDA) - Used when more than 2 response classes 
multinomial Logistic Regression is also available. Use LDA when:
* classes are well separated - LDA can be more stable
* n is small and predictors X are approx normal in each of the classes - LDA can be more stable
* more than 2 response classes

#### 4.4.1, 4.4.2, 4.4.3
LDA assumes that the class k density function of x f(x) is ~ normal(mu_k, sigma^2); further the variances (sigma^2) are the same for all classes
For p-dimensional random vector X is multitivariate N(mu_k, SIGMA), where SIGMA is the covariance matrix common to all classes

#### 4.4.4 Quadratic Discriminant Analysis (QDA)
QDA relaxes the assumption that SIGMA (the covarance matrix) is the same for all k classes
thus X ~ multivariant N(mu_k, SIGMA_k)

### 4.6 Lab: Logisitc Regression, LDA, QDA, kNN
Where I can, I'm goint to use the caret package and tidyverse
#### 4.6.1 Stock Market Data
#### 4.6.2 Logistic Regression
```{r}
Smarket <- as.tibble(Smarket)
Smarket_trn <-  Smarket %>% 
                filter(Year < 2005)
Smarket_tst <- Smarket %>% 
                filter(Year >= 2005)
lm_fit <- Smarket_trn %>%  
                train(Direction ~ Lag1 + Lag2, method = "glm", family = binomial, data = .)
summary(lm_fit)   
```
```{r}
lm_prob <- predict(lm_fit, newdata = Smarket_tst, type = "prob")
lm_prob
```
```{r}
lm_pred <- lm_prob %>% 
                mutate(Direction = if_else(Up > 0.5, "Up", "Down"))
lm_pred
```
```{r}
lm_tab <- table(predicted = lm_pred$Direction,actual = Smarket_tst$Direction)
lm_tab
confusionMatrix(lm_tab, positive = "Up")
```
```{r}
calc_class_err <- function(actual, predicted) {
  mean(actual != predicted)
}
```
```{r}
calc_class_err(actual = Smarket_tst$Direction, predicted = lm_pred$Direction)
```
#### 4.6.3 Linear Discriminant Analysis - LDA
```{r}
# Train and test data sets are as before in LDA. Ready to use
lda_fit <- Smarket_trn %>% 
          train(Direction ~ Lag1 + Lag2, method = "lda", data = .)
names(lda_fit)

```
```{r}
# the model info is in $finalModel. 
lda_fit$finalModel
```


```{r}
# note type = "prob" returns the posterior probabilities. You can use this information to adjust the cutoff for classification like I did for the logistic model. # that is I can make the cutoff for an "Up" to greater that some given probability.

lda_pred_prob <- predict(lda_fit, newdata = Smarket_tst, type = "prob")
lda_pred_prob

```

```{r}
# the max probability for a prediced Up is 0.54
max(lda_pred_prob$Up)
```

```{r}
# note type = "raw" returns the predicted Direction. It automatically uses a 0.5 cutoff
lda_pred_raw <- predict(lda_fit, newdata = Smarket_tst, type = "raw")
lda_pred_raw
```
```{r}
lda_tab <- table(predicted = lda_pred_raw,actual = Smarket_tst$Direction)
confusionMatrix(lda_tab, positive = "Up")
# results are the same as the logistic regression fit
```
```{r}
calc_class_err(predicted = lda_pred_raw,actual = Smarket_tst$Direction)
```
#### Quadratic Discriminant Analysis - QDA
```{r}
# fitting a qda to the stock market data. using the same train and test sets
qda_fit <- Smarket_trn %>% 
          train(Direction ~ Lag1  + Lag2, method = "qda", data = .)
qda_fit$finalModel
```
```{r}
qda_raw <- predict(qda_fit, newdata = Smarket_tst, type = "raw")
qda_raw
```
```{r}
qda_tab <- table(predicted = qda_raw, actual = Smarket_tst$Direction)
qda_conf <- confusionMatrix(qda_tab, positive = "Up")
qda_conf
```
```{r}
calc_class_err(predicted = qda_raw, actual = Smarket_tst$Direction)
```
#### 4.6.5 K- Nearest Neighbors - kNN
```{r}
Smarket_trn
```
```{r}
# I'm using trainControl() and train() from the caret package instead of the what's in the text
ctlr <- trainControl(method = "cv", number = 10) # using cross-validtion w/ 10 folds
# fitting kNN to training set and centering and scaling the predictors, 
knn_fit <- Smarket_trn %>% 
                    train(Direction ~ Lag1 + Lag2, data = ., method = "knn", trControl = ctlr, preProcess = c("center", "scale"), tuneLength = 20, na.action = "na.omit", metric = "Kappa")
print(knn_fit) # kNN fitting results; note training() used k nearest neighbors from 5 - 43 and then used k = 33 for the final fitting
```
```{r}
plot(knn_fit) # plotting Kappa vs #Neighbors
```
```{r}
# testing the fitted model on the test data using predict()
# building a pred vs actual table to feed to confusionMatrix()
knn_pred <- predict(knn_fit, newdata = Smarket_tst, type = "raw")
knn_pred_tab <- table(actual = Smarket_tst$Direction, predicted = knn_pred)
knn_conf <- confusionMatrix(knn_pred_tab, positive = "Up")
knn_conf
```
```{r}
calc_class_err(actual = Smarket_tst$Direction, predicted = knn_pred)
```
#### 4.6.6 kNN - importance of scaling/standardizing predictors with widly varing scales (e.g. age and salary)
```{r}
Caravan_scaled <- Caravan %>% 
                  mutate_if(is.numeric, funs(scale(.)))
head(Caravan_scaled)
```
```{r}
# taking a 80% sample for training
sample_index <- sample(nrow(Caravan_scaled), trunc(.80 * nrow(Caravan_scaled)), replace = FALSE)
head(sample_index)
head(Caravan_scaled)
Caravan_trn <- Caravan%>% # scaling loses the row numbering
               filter(row_number() %in% sample_index)
Caravan_tst <- Caravan %>% 
                filter(!row_number() %in% sample_index)
    ```
    
```{r}
# I'm using trainControl() and train() from the caret package instead of the what's in the text
ctlr <- trainControl(method = "cv", number = 10) # using cross-validtion w/ 10 folds
# fitting kNN to training set and centering and scaling the predictors, 
knn_fit <- Caravan_trn %>% 
                    train(Purchase ~ ., data = ., method = "knn", trControl = ctlr, preProcess = c("center", "scale"), tuneLength = 20, na.action = "na.omit", metric = "Kappa")
print(knn_fit) # kNN fitting results; note training() used k nearest neighbors from 5 - 43 and then used k = 7 for the final fitting based on Kappa
```
```{r}
plot(knn_fit) # plotting Kappa vs #Neighbors
```
```{r}
# testing the fitted model on the test data using predict()
knn_pred <- predict(knn_fit, newdata = Caravan_tst)
# building a pred vs actual table to feed to confusionMatrix()
knn_pred_tab <- table(predicted = knn_pred, actual = Caravan_tst$Purchase)
knn_conf <- confusionMatrix(knn_pred_tab, positive = "Yes")
knn_conf
```
```{r}
calc_class_err(actual = Caravan_tst$Purchase, predicted = knn_pred)
```
### 4.7 Exercises - Applied
#### 10
```{r}
Weekly <- as.tibble(Weekly)
Weekly
```
##### 10.a
```{r}
ggpairs(Weekly, progress = FALSE)
```
##### 10b
```{r}
log_fit <- Weekly %>% 
          train(Direction ~. - Year - Today, method = "glm", family = binomial, data = .)
summary(log_fit)    
```
none of the predictors seem signifant in the full model. 
##### 10c
```{r}
log_pred <- Weekly %>%  
            predict(log_fit, data = ., type = "raw")
log_tab <- table(predicted = log_pred, actual = Weekly$Direction)
log_conf <- confusionMatrix(log_tab, positive = "Up")
log_conf
```
```{r}
1 - calc_class_err(predicted = log_pred, actual = Weekly$Direction)
```
##### 10d - Log Regression
```{r}
# split the data into training and test sets
Weekly_trn <- Weekly %>% 
              filter(Year <= 2008)
Weekly_tst <- Weekly %>% 
              filter(Year > 2008)
# log model on training set using only Lag2 as the predictor of direction
log_fit_2 <- Weekly_trn %>% 
            train(Direction ~ Lag2, method = "glm", family = binomial, data = .)
summary(log_fit_2)
```
```{r}
log_pred_2 <- predict(log_fit_2, newdata = Weekly_tst, type = "raw")
log_tab_2 <- table(predicted = log_pred_2, actual = Weekly_tst$Direction)
log_conf_2 <- confusionMatrix(log_tab_2, positive = "Up")
log_conf_2
```
```{r}
calc_class_err(predicted = log_pred_2, actual = Weekly_tst$Direction)
```
##### 10e - LDA
```{r}
lda_fit <- Weekly_trn %>% 
           train(Direction ~ Lag2, method = "lda", data = .)
lda_fit$finalModel # for LDA the summary is in $final$Model
```
```{r}
lda_pred <- predict(lda_fit, newdata = Weekly_tst, type = "raw") # getting the classification results rather than the probs
lda_tab <- table(predicted = lda_pred, actual = Weekly_tst$Direction)
lda_conf <- confusionMatrix(lda_tab, positive = "Up")
lda_conf
```
```{r}
calc_class_err(predicted = lda_pred, actual = Weekly_tst$Direction)
```
Note logistic regression and LDA return the same results. Not surprising since it's a 2-class outcome

##### 10f - QDA
```{r}
qda_fit <- Weekly_trn %>% 
            train(Direction ~ Lag2, method = "qda", data = .)
qda_fit$finalModel
```
```{r}
qda_pred <- predict(qda_fit, newdata = Weekly_tst, type = "raw")
qda_tab <- table(predicted = qda_pred, actual = Weekly_tst$Direction)
qda_conf <- confusionMatrix(qda_tab, positive = "Up")
qda_conf
```
```{r}
calc_class_err(predicted = qda_pred, actual = Weekly_tst$Direction)
```
QDA does worse that logistic regression and LDA. Probably due to overfitting

##### 10g - kNN
```{r}
# need to use tuneGrid() to force a single k value
 knn_fit <- Weekly_trn %>% 
           train(Direction ~ Lag2, method = "knn", tuneGrid = data.frame(k = 1), data = .) 
knn_fit$finalModel
```
```{r}
knn_pred <- predict(knn_fit, newdata = Weekly_tst, type = "raw")
knn_tab <- table(predict = knn_pred, actual = Weekly_tst$Direction)
knn_conf <- confusionMatrix(knn_tab, positive = "Up")
knn_conf
```
```{r}
calc_class_err(predict = knn_pred, actual = Weekly_tst$Direction)
```
kNN with k = 1 is the worst of the models
```{r}
# letting train find the best kNN 
knn_fit <- Weekly_trn %>% 
                    train(Direction ~ Lag2, data = ., method = "knn", trControl = ctlr, preProcess = c("center", "scale"), tuneLength = 50, na.action = "na.omit", metric = "Kappa")
print(knn_fit)
```
```{r}
plot(knn_fit)
```

```{r}
knn_pred <- predict(knn_fit, newdata = Weekly_tst, type = "raw")
knn_tab <- table(predicted = knn_pred, actual = Weekly_tst$Direction)
knn_conf <- confusionMatrix(knn_tab, positive = "Up")
knn_conf
```

logistic, LDA and QDA still outperform kNN in this case
### 9.1 Visualiztion for Classification
Working thru the Chapter 9 in the supplemental R for Statistical Learning
```{r}
# Splitting the data into a test set and training set
index <- seq(1:nrow(Default))
Default <- Default %>% 
  bind_cols(index = index, Default)
Default <- Default %>% 
           dplyr::select(index, default, student, balance, income)
Default
default_idx <- sample(nrow(Default), 5000)
default_trn <- Default[default_idx,]
default.tst <- Default[-default_idx,]
default_trn
default.tst
```
```{r}
featurePlot(x = default_trn[, c("balance", "income")],
            y = default_trn$default,
            plot = "density",
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2,1),
            auto.key = list(columns = 2))
            
```
```{r}
featurePlot(x = default_trn[, c("balance", "income")],
            y = default_trn$student,
            plot = "density",
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2,1),
            auto.key = list(columns = 2))
```
```{r}
featurePlot(x = default_trn[, c("student", "balance", "income")], 
            y = default_trn$default, 
            plot = "pairs",
            auto.key = list(columns = 2))
```
```{r}
library(ellipse)
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default, 
            plot = "ellipse",
            auto.key = list(columns = 2))
```
### 9.2 A Simple Classifier
```{r}
# building a very simple classifier for default with default as the predictor based on eye-balling the plotts above
# this classifier function takes x and boundary vales as inputs and assigns a 1 if above the boundary, else it assigns a 0
simple_class <- function(x, boundary, above = 1, below = 0){
  ifelse(x > boundary, above, below)
}
# now calling the function on the training and test sets
default_trn_pred <- simple_class(x = default_trn$balance, boundary = 1400, above = "Yes", below = "No")
head(default_trn_pred)
sum(default_trn_pred == "Yes") # number of predicted defaults based on balance > 1400
sum(default_trn$default == "Yes") # actual number of defaults in traing set

default_tst_pred <- simple_class(x = default.tst$balance, boundary = 1400, above = "Yes", below = "No")
head(default_tst_pred)
sum(default_tst_pred == "Yes") # number of predicted defaults based on balance > 1400
sum(default.tst$default == "Yes") # actual number of defaults in test set

```
### 9.3 metrics for Classification
```{r}
# building a cross table
trn_tab <- table(predicted = default_trn_pred, actual = default_trn$default)
trn_tab
```
```{r}
# using the confusionMatrix() from the "caret" package. Feed it a table and specify the "positive" classification state; in this case "Yes"
# need package "e1071" - it was not loaded automatically by "caret"
library(e1071)
trn_con_mat <- confusionMatrix(trn_tab, positive = "Yes")
names(trn_con_mat)
trn_con_mat
```
Sensitivity = True Positive Rate = TP/P = TP/(TP + FN)
```{r}
# calculating the Sensitivity by hand and comparing it to the confusion matrix above
sens <- 144/(32+144)
sens
```
# Specificity = True Negative Rate = TN/N = TN/(TN + FP)
```{r}
# calculating the Specificity by hand and comparing it to the confusion matrix above
spe <- 4322/(4322+502)
spe
```
So, how good is the simple classifier? In the training set only 0.0352 of the observations have a default status =  "Yes". This is the "Prevalance" of the positive claiification state in the set. The classification error rate is the "sum of off-diagonal"/ total observations. Or 1-Accuracy in the confusion matrix stats.
```{r}
prev <- (32+144)/5000
prev
err <- (32 + 502)/5000
err # the error rate is 10%. In this case, a better classifer rule would be to simple classify all observations as "No." The max error rate would then be = prevalance =.0352
```
```{r}
# rerunning the simple classifer w/ balance set above the max balance in the training data set to force all to "No" except for 1"
# bulding the table and confusion matrix again
max(default_trn$balance)
default_trn_pred_2 <- simple_class(x = default_trn$balance, boundary = 2578, above = "Yes", below = "No")
trn_tab_2 <- table(predicted = default_trn_pred_2, actual = default_trn$default)
trn_tab_2
trn_con_mat_2 <- confusionMatrix(trn_tab_2, positive = "Yes")
trn_con_mat_2
```
```{r}
err_2 <- 1 - trn_con_mat_2$overall["Accuracy"]
err_2
```
This naive model has a better error rate (3.5%) that the simple classifier using balance > 1400 (10.7%)

### 10 Logistic Regression
Working thru the Chapter 10 in the supplemental R for Statistical Learning
```{r}
default_trn
```
```{r}
# default status into 0,1 for linear regression. 
default_trn_lm <- default_trn
default_trn_lm <- default_trn_lm %>% 
                   mutate(default = if_else(default == "No", 0, 1)) 
default_trn_lm
default_tst_lm <- default.tst
default_tst_lm <- default_tst_lm %>% 
                   mutate(default = if_else(default == "No", 0, 1)) 
default_tst_lm
```
```{r}
# showing why linear regression doesn't work well for classification of qualitative variables
model_lm = lm(default ~ balance, data = default_trn_lm)
summary(model_lm)
```
On the face, the summary indiates that the lm is a good fit
```{r}
default_trn_lm %>% 
  ggplot(aes(balance, default)) +
  geom_hline(aes(yintercept = 0)) +
  geom_hline(aes(yintercept = 1)) +
  geom_hline(aes(yintercept = 0.5), linetype = 2) +
  geom_point(color = "darkorange", shape = "|") +
  geom_smooth(method = "lm", color = "dodgerblue", se = FALSE) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), color = "green", se = FALSE) +
  labs(title = "Using Linear Regression for Classification of Qualitative Variable", subtitle = "Not a Good Idea", y = "Probability of Default", x = "Credit Card Balance", caption = "Linear Regression line runs from below 0 and doesn't approach 1; Logistic Regression line between 0 & 1")
```
### 10.3 Logistic Regression - examples from the supplemental book
```{r}
# stepping thru fitting a logit model
model_glm <- default_trn %>% 
              glm(default ~ balance, data = ., family = "binomial") # glm can fit several "types"" of family = "binomial" fits a logit
summary(model_glm)
```
```{r}
# use predict() to return the probabilities and then classify default = Yes if pr(x) > 0.5
model_glm_pred <- ifelse(predict(model_glm, type = "response")  > 0.5, "Yes", "No")
train_tab_glm <- table(predicted = model_glm_pred, actual = default_trn$default)
train_conf_glm <- confusionMatrix(train_tab_glm, positive = "Yes")
train_conf_glm
```
```{r}
# logit model error rate
err_glm <- 1 - train_conf_glm$overall["Accuracy"]
err_glm <- tibble(err_glm)
err_glm
```
Note that on the training set that the classifier error rate (0.0262) is slighly lower that the Prevalance (0.0338), so it's a little better than classifying defauls status = "No" regardless of balance
Sensitivity = TP/P = 50/(114+50) = 0.3049
Specificity = TN/N = 4819/(4819+17) = .9965
```{r}
# lets see how the fitted model does on the test set
model_glm_pred_2 <- ifelse(predict(model_glm, newdata =  default.tst,type = "response")  > 0.5, "Yes", "No")
test_tab_glm <- table(predicted = model_glm_pred_2, actual = default.tst$default)
test_conf_glm <- confusionMatrix(test_tab_glm, positive = "Yes")
test_conf_glm
```
```{r}
# logit model error rate
err_glm_2 <- 1 - test_conf_glm$overall["Accuracy"]
err_glm_2 <- tibble(err_glm_2)
err_glm_2
```
The classifier has lower (good) metrics on the test set. This is expected since the fitted model was optimized for the training set
```{r}
# building function to calculate the classification error rate
calc_class_err <- function(actual, predicted){
  mean(actual != predicted)
}
calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```
### 10.4 ROC Curves - Sweeps thru Probability Cutoffs 
```{r}
# need to load the "pROC" package
library(pROC)
test_prob = predict(model_glm, newdata = default.tst, type = "response") # 
test_roc = roc(default.tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```
```{r}
# use coords() to fine the "best" threshold setting for the model's cutoff probability
coords(test_roc, x = "best", ret = "threshold", best.method = "closest.topleft")
```
```{r}
# lets run the trainig and test models again w/ the new threshold set at 0.036047
model_glm_pred <- ifelse(predict(model_glm, type = "response")  > 0.03036047, "Yes", "No")
train_tab_glm <- table(predicted = model_glm_pred, actual = default_trn$default)
train_conf_glm_2 <- confusionMatrix(train_tab_glm, positive = "Yes")
train_conf_glm_2
```
### 10.5 - Multinomial Logistic Regression - used when the response variable has more than 3 categories
```{r}
data("iris")
head(iris)
# splitting data set into train and test
iris_obs <- nrow((iris))
iris_idx <- sample(iris_obs, size = trunc(0.40 * iris_obs), replace = FALSE)
iris_trn <- iris[iris_idx,]
iris_tst <- iris[-iris_idx,]
iris_trn
iris_tst
```
```{r}
# using multinom() from the nnet package
library(nnet)
model_multi <- iris_trn %>% 
  multinom(Species ~ ., data = . , trace = FALSE)
summary(model_multi)
```
```{r}
# lets see how well it did on the training data
model_multi_pred_trn <- predict(model_multi, newdata = iris_trn)
iris_mult_trn_tbl <- table(predicted = model_multi_pred_trn, actual = iris_trn$Species)
iris_mult_trn_tbl
```
```{r}
# now run the model on the test data
model_multi_pred_tst <- predict(model_multi, newdata = iris_tst)
iris_mult_tst_tbl <- table(predicted = model_multi_pred_tst, actual = iris_tst$Species)
iris_mult_tst_tbl
```
Not too bad - misclassified 4 out of 75

## Generative Models

```{r}
# doing some plots. These plots are not as easy to do in ggplot
iris
iris_trn %>%
  caret::featurePlot(x = iris_trn[,c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")],
                     y = iris_trn$Species,
                     plot = "density",
                     scales = list(x = list(relation = "free"),
                                   y = list(relation = "free")),
                     adjust = 1.5,
                     pch = "|",
                     layout = c(2,2),
                     auto.key = list(columns = 3))
  
```

```{r}
caret::featurePlot(x = iris_trn[, c("Sepal.Length", "Sepal.Width", 
                                    "Petal.Length", "Petal.Width")], 
                   y = iris_trn$Species,
                   plot = "ellipse",
                   auto.key = list(columns = 3))
```
```{r}
caret::featurePlot(x = iris_trn[, c("Sepal.Length", "Sepal.Width", 
                                    "Petal.Length", "Petal.Width")], 
                   y = iris_trn$Species,
                   plot = "box",
                   scales = list(y = list(relation = "free"),
                                 x = list(rot = 90)),
                   layout = c(4, 1))
```
```{r}
# this is brute forcing it. 
p1 <- iris_trn %>% 
  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point() +
  stat_ellipse()
p2 <- iris_trn %>% 
  ggplot(aes(Sepal.Length, Petal.Length, color = Species)) +
  geom_point() +
  stat_ellipse()
p3 <- iris_trn %>% 
  ggplot(aes(Sepal.Length, Petal.Width, color = Species)) +
  geom_point() +
  stat_ellipse()
p4 <- iris_trn %>% 
  ggplot(aes(Sepal.Width, Petal.Length, color = Species)) +
  geom_point() +
  stat_ellipse()
p5 <- iris_trn %>% 
  ggplot(aes(Sepal.Width, Petal.Width, color = Species)) +
  geom_point() +
  stat_ellipse()
p6 <- iris_trn %>% 
  ggplot(aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  stat_ellipse()
grid.arrange(p1,p2,p3,p4,p5,p6, nrow = 2)
```
#### 11.1 Linear Discriminant Analysis - predictors are multivariate normal conditioned on the classes and the variance for all classes is the same
X|Y = k ~ N(mu_k, sigma^2): Note that the sigma^2 is the covarance Matrix, and that the Xi's may be correlated
```{r}
# fitting an LDA model to the iris training data
iris_lda <- lda(Species~., data = iris_trn)
iris_lda
```
```{r}
# using the predict() on training and test data
iris_lda_trn_pred <- predict(iris_lda, iris_trn)$class
iris_lda_tst_pred <- predict(iris_lda, iris_tst)$class
calc_class_err(actual = iris_trn$Species, predicted = iris_lda_trn_pred)
calc_class_err(predicted = iris_lda_tst_pred, actual = iris_tst$Species)
```
```{r}
table(predicted = iris_lda_tst_pred, actual = iris_tst$Species)
```
#### Quadratic Discriminant Analysis - predictors are multivariate normal conditioned on the classes and the variance for each class may be different
X|Y = k ~ N(mu_k, sigma_k^2). Note that the sigma_k^2 is the covarance Matrix for class k, and that the Xi's in class k may be correlated
```{r}
# fitting an QDA model to the iris training data
iris_qda <- qda(Species~., data = iris_trn)
iris_qda
```
```{r}
# using the predict() on training and test data
iris_qda_trn_pred <- predict(iris_qda, iris_trn)$class
iris_qda_tst_pred <- predict(iris_qda, iris_tst)$class
calc_class_err(predicted = iris_qda_trn_pred, actual = iris_trn$Species)
calc_class_err(predicted = iris_qda_tst_pred, actual = iris_tst$Species)
```
```{r}
table(predicted = iris_qda_tst_pred, actual = iris_tst$Species)
```
#### 11.3 Naive Bayes - predictors are multivariate normal conditioned on the classes and the variance for each class may be different
X|Y = k ~ N(mu_k, sigma_k^2). Note that the sigma_k^2 is the covarance Matrix for class k, and that the Xi's in all classes are not correlated - that is they are independent
Use Naive Bayes when there are a large number of predictors with a limited sample size
```{r}
library(e1071)
iris_nb = naiveBayes(Species ~ ., data = iris_trn)
iris_nb
```
```{r}
# using the predict() on traning and test data
iris_nb_trn_pred = predict(iris_nb, iris_trn)
iris_nb_tst_pred = predict(iris_nb, iris_tst)
```

```{r}
calc_class_err(predicted = iris_nb_trn_pred, actual = iris_trn$Species)
```
```{r}
calc_class_err(predicted = iris_nb_tst_pred, actual = iris_tst$Species)
```

```{r}
table(predicted = iris_nb_tst_pred, actual = iris_tst$Species)
```
#### 11.4 Discrete Inputs

```{r}
# making Pedal.Width a categorical predictor
iris_trn_mod <- iris_trn
iris_trn_mod <- iris_trn %>%
                mutate(Sepal.Width = if_else(Sepal.Width > 4, "Large", if_else(Sepal.Width > 3, "Medium", "Small"))) 
      
iris_trn_mod
iris_tst_mod <- iris_tst
iris_tst_mod <- iris_tst %>%
                mutate(Sepal.Width = if_else(Sepal.Width > 4, "Large", if_else(Sepal.Width > 3, "Medium", "Small"))) 
      
iris_tst_mod
```
```{r}
# running naiveBayes()
iris_trn_cat_nb <-  iris_trn_mod %>% 
                    naiveBayes(Species ~ Sepal.Length + Sepal.Width, data = .)
iris_trn_cat_nb
```
```{r}
# using the predict() on traning and test data
iris_nb_trn_cat_nb_pred = predict(iris_trn_cat_nb, iris_trn_mod)
iris_nb_tst_cat_nb_pred = predict(iris_trn_cat_nb, iris_tst_mod)
```
```{r}
table(predict = iris_nb_trn_cat_nb_pred, actual = iris_trn_mod$Species)
```
```{r}
calc_class_err(actual =iris_trn_mod$Species, predicted = iris_nb_trn_cat_nb_pred)
```

```{r}
table(predict = iris_nb_tst_cat_nb_pred, actual = iris_tst_mod$Species)
```
```{r}
calc_class_err(actual =iris_tst_mod$Species, predicted = iris_nb_tst_cat_nb_pred)
```
```{r}
# splitting the data into train and test sets. This may me the easist way
sample_index <- sample(nrow(iris), trunc(.50 * nrow(iris)), replace = FALSE)
iris_trn_2 <- iris %>% 
              filter(row_number() %in% sample_index)
iris_tst_2 <- iris %>% 
              filter(!row_number() %in% sample_index)
iris_tst_2
```
```{r}
# an altenative way, but not recommended 
iris_trn_3 <- iris %>% sample_frac(.5, replace = FALSE)
iris_tst_3 <- iris %>%
              anti_join(iris_trn_3)
iris_tst_3
```

## Chapter 12 k-Nearest Neighbors: Non-parametric Classification
### 12.1 Binary Classification/Outcome Variables
```{r}
head(Default)
# changing student status to 0 & 1 for kNN. kNN measures distance so predictors should be numeric
Default_knn <- Default %>% 
               mutate(student = if_else(student == "No", 0, 1 ))
Default_knn
```
```{r}
# split the data into training and test data sets. here I'm making a 70 30 split
Default_knn
sample_index <- sample(nrow(Default_knn), trunc(0.7 * nrow(Default_knn), replace = FALSE))
Default_knn_trn <- Default_knn %>% 
                   filter(row_number() %in% sample_index)
head(Default_knn_trn)
Default_knn_trn <- mutate_if(Default_knn_trn,is.numeric, funs(scale(.)))
head(Default_knn_trn)
Default_knn_tst <- Default_knn %>% 
                   filter(!row_number() %in% sample_index) %>% 
                   mutate_if(is.numeric, funs(scale(.)))
Default_knn_tst
```
```{r}
# I'm using trainControl() and train() from the caret package instead of the what's in the text
ctlr <- trainControl(method = "repeatedcv", number = 10, repeats = 10) # using cross-validtion w/ 10 folds
# fitting kNN to training set and centering and scaling the predictors, 
Default_knn_fit <- Default_knn_trn %>% 
                    train(default ~., data = ., method = "knn", trControl = ctlr, preProcess = c("center", "scale"), tuneLength = 20, na.action = "na.omit")
print(Default_knn_fit) # kNN fitting results; note training() used k nearest neighbors from 5 - 43 and then used k = 31 for the final fitting
```
```{r}
plot(Default_knn_fit) # plotting Accuracy vs #Neighbors
```

```{r}
# testing the fitted model on the test data using predict()
Default_knn_pred <- predict(Default_knn_fit, newdata = Default_knn_tst)
# building a pred vs actual table to feed to confusionMatrix()
Default_knn_pred_tab <- table(actual = Default_knn_tst$default, predicted = Default_knn_pred)
Default_knn_conf <- confusionMatrix(Default_knn_pred_tab, positive = "Yes")
Default_knn_conf
```
```{r}
calc_class_err(actual = Default_knn_tst$default, predicted = Default_knn_pred)
```
prediction error on the test set (0.024) is higher that the prevalence of defaults = Yes (0.013). Note alos that Accuracy (0.976) is less than NIR (0.9867)!
So, in this case kNN is worse (in terms of accuracy) than just classifying default status "No" for everyone
```{r}
# type II error is important in this case - want to avoid missing those that may default
typeII <- 1 - Default_knn_conf$byClass["Specificity"]
typeII
```

#### 12.2 Categorical Data - kNN and multi-class 

```{r}
# using the iris data set, sepearting it into training and test sets and scalling both
iris_sample <- sample(nrow(iris), trunc(0.5 * nrow(iris)), replace = FALSE)
iris_knn_trn <- iris %>% 
                filter(row_number() %in% iris_sample) %>% 
                mutate_if(is.numeric, funs(scale(.)))
iris_knn_tst <- iris %>%
                filter(!row_number() %in% iris_sample) %>% 
                mutate_if(is.numeric, funs(scale(.)))
```
```{r}
ctlr <- trainControl(method = "repeatedcv", number = 10, repeats = 3) # using cross-validtion w/ 10 folds
# fitting kNN to training set 
iris_knn_fit <- iris_knn_trn %>% 
                    train(Species ~., data = ., method = "knn", trControl = ctlr, tuneLength = 20, na.action = "na.omit")
print(iris_knn_fit) # kNN fitting results; note training() used k nearest neighbors from 5 - 43 and then used k = 31 for the final fitting
```
```{r}
plot(iris_knn_fit)
```
```{r}
# running fitted model on test data
iris_knn_pred <- predict(iris_knn_fit, newdata = iris_knn_tst)
iris_knn_pred_tab <- table(actual = iris_knn_tst$Species, predicted = iris_knn_pred)
iris_knn_conf <- confusionMatrix(iris_knn_pred_tab)
iris_knn_conf
```




