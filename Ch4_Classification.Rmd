---
title: "Ch4_Classiication_Problems"
author: "G Smith"
date: "October 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

## 4 Classification

The response value is qualitative and takes discrete values in an "unordered" set
```{r}
# Loading my standard work environment libraries and the ISLR library
library(MASS) # LDA package
library(class) # K-Nearest Neighbors package
library(broom)
library(gridExtra)
library(GGally)
library(knitr)
library(ISLR)
library(car)
library(caret)
library(tidyverse)
# note that dplyr select() is masked by MASS library. Need to use dplyr::select()
```

### 4.1 Introduction to Classification Problems
```{r}
# loading the "Default" data set used in the section
data(Default)
Default <- as.tibble(Default)
Default
```
```{r}
# plotting Balance vs Income and Default vs Balance
Default %>% 
  sample_n(1500, replace = FALSE) %>%   # pulling a sample of 1500 for the plot
  ggplot(aes(balance, income, color = default, shape = default)) +
  geom_point()
```
Obseravtions: there are many fewer "Yes" compared to "No" - the prior probablity of a "Yes" is much lower than a "No"
higher balances seem to have more "Yes" defaults. Doesn't appear to be a default pattern based on income
```{r}
# box plots of dafault vs balance and income
# putting boxplots side by side
p1 <-  Default %>% 
  ggplot(aes(x = default, y = balance, color = default)) +
  geom_boxplot()
p2 <- Default %>% 
  ggplot(aes(x = default, y = income, color = default)) +
  geom_boxplot()
grid.arrange(p1, p2, nrow = 1)
```
Observations: there appears a strong pattern between default status and balance; not so for default status and income. 
### 4.3 Logistic Regression - Used where there are 2 Response Classes 
```{r}
# building my own logistic function and plotting it
b0 <- -3.5
b1 <- 0.1
x <- seq(1:100)
p <- exp(b0 + b1 * x)/(1+exp(b0 + b1 * x))
my_log <- tibble(x = x,p = p)
my_log
my_log %>% 
ggplot(aes(x,p)) +
  geom_line()

```
```{r}
# plotting default where the predictor is qualitative - in this case student status
Default %>% 
  ggplot(aes(x = default, y = student, color = student)) +
  geom_count(aes(size = stat(prop), group = student)) +
  scale_size_area(max_size = 10)
```
4.3.1 The Logistic Model
```{r}
# logistic regression examples from the text book
# logistic regression w/ a continous predictor
log.fit.1 <- Default %>% 
  glm(default ~ balance, data = ., family = "binomial")
tidy(log.fit.1)
```
```{r}
summary(log.fit.1)
```
### 4.3.3
```{r}
# logistic regression w/ a discrete predictor
log.fit.2 <- Default %>% 
  glm(default ~ student, family = "binomial", data = .)
tidy(log.fit.2)
```
```{r}
summary(log.fit.2)
```
### 4.3.4 Multiple Logistic Regression
```{r}
log.fit.3 <- Default %>% 
  glm(default ~ balance + income + student, family = "binomial", data = .)
tidy(log.fit.3)
```
```{r}
summary(log.fit.3)
```
```{r}
# plots of student status vs income and balance
p1 <- Default %>% 
      ggplot(aes(student, income, color = student)) +
      geom_boxplot()
p2 <- Default %>% 
      ggplot(aes(student, balance, color = student)) +
      geom_boxplot()
grid.arrange(p1,p2, nrow = 1)
```
```{r}
p1 <- Default %>%
  filter(student == "Yes") %>% 
  ggplot(aes(balance,income, color = default, shape = student)) +
  geom_point()
p2 <- Default %>%
  filter(student == "No") %>% 
  ggplot(aes(balance,income, color = default, shape = student)) +
  geom_point()
grid.arrange(p1,p2,ncol = 1)
```
```{r}
p1 <- Default %>%
  filter(student == "Yes") %>% 
  ggplot(aes(balance)) +
  geom_density() +
  facet_grid(default ~.)
p2 <- Default %>%
  filter(student == "No") %>% 
  ggplot(aes(balance)) +
  geom_density() + 
  facet_grid(default ~.)
grid.arrange(p1,p2,ncol = 1)
```

### 9.1 Visualiztion for Classification
Working thru the Chapter 9 in the supplemental R for Statistical Learning
```{r}
# Splitting the data into a test set and training set
index <- seq(1:nrow(Default))
Default <- Default %>% 
  bind_cols(index = index, Default)
Default <- Default %>% 
           dplyr::select(index, default, student, balance, income)
Default
default_idx <- sample(nrow(Default), 5000)
default_trn <- Default[default_idx,]
default.tst <- Default[-default_idx,]
default_trn
default.tst
```
```{r}
featurePlot(x = default_trn[, c("balance", "income")],
            y = default_trn$default,
            plot = "density",
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2,1),
            auto.key = list(columns = 2))
            
```
```{r}
featurePlot(x = default_trn[, c("balance", "income")],
            y = default_trn$student,
            plot = "density",
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2,1),
            auto.key = list(columns = 2))
```
```{r}
featurePlot(x = default_trn[, c("student", "balance", "income")], 
            y = default_trn$default, 
            plot = "pairs",
            auto.key = list(columns = 2))
```
```{r}
library(ellipse)
featurePlot(x = default_trn[, c("balance", "income")], 
            y = default_trn$default, 
            plot = "ellipse",
            auto.key = list(columns = 2))
```
### 9.2 A Simple Classifier
```{r}
# building a very simple classifier for default with default as the predictor based on eye-balling the plotts above
# this classifier function takes x and boundary vales as inputs and assigns a 1 if above the boundary, else it assigns a 0
simple_class <- function(x, boundary, above = 1, below = 0){
  ifelse(x > boundary, above, below)
}
# now calling the function on the training and test sets
default_trn_pred <- simple_class(x = default_trn$balance, boundary = 1400, above = "Yes", below = "No")
head(default_trn_pred)
sum(default_trn_pred == "Yes") # number of predicted defaults based on balance > 1400
sum(default_trn$default == "Yes") # actual number of defaults in traing set

default_tst_pred <- simple_class(x = default.tst$balance, boundary = 1400, above = "Yes", below = "No")
head(default_tst_pred)
sum(default_tst_pred == "Yes") # number of predicted defaults based on balance > 1400
sum(default.tst$default == "Yes") # actual number of defaults in test set

```
### 9.3 metrics for Classification
```{r}
# building a cross table
trn_tab <- table(predicted = default_trn_pred, actual = default_trn$default)
trn_tab
```
```{r}
# using the confusionMatrix() from the "caret" package. Feed it a table and specify the "positive" classification state; in this case "Yes"
# need package "e1071" - it was not loaded automatically by "caret"
library(e1071)
trn_con_mat <- confusionMatrix(trn_tab, positive = "Yes")
names(trn_con_mat)
trn_con_mat

```
Sensitivity = True Positive Rate = TP/P = TP/(TP + FN)
```{r}
# calculating the Sensitivity by hand and comparing it to the confusion matrix above
sens <- 144/(32+144)
sens
```
# Specificity = True Negative Rate = TN/N = TN/(TN + FP)
```{r}
# calculating the Specificity by hand and comparing it to the confusion matrix above
spe <- 4322/(4322+502)
spe
```
So, how good is the simple classifier? In the training set only 0.0352 of the observations have a default status =  "Yes". This is the "Prevalance" of the positive claiification state in the set. The classification error rate is the "sum of off-diagonal"/ total observations. Or 1-Accuracy in the confusion matrix stats.
```{r}
prev <- (32+144)/5000
prev
err <- (32 + 502)/5000
err # the error rate is 10%. In this case, a better classifer rule would be to simple classify all observations as "No." The max error rate would then be = prevalance =.0352
```
```{r}
# rerunning the simple classifer w/ balance set above the max balance in the training data set to force all to "No" except for 1"
# bulding the table and confusion matrix again
max(default_trn$balance)
default_trn_pred_2 <- simple_class(x = default_trn$balance, boundary = 2578, above = "Yes", below = "No")
trn_tab_2 <- table(predicted = default_trn_pred_2, actual = default_trn$default)
trn_tab_2
trn_con_mat_2 <- confusionMatrix(trn_tab_2, positive = "Yes")
trn_con_mat_2
```
```{r}
err_2 <- 1 - trn_con_mat_2$overall["Accuracy"]
err_2
```
This naive model has a better error rate (3.5%) that the simple classifier using balance > 1400 (10.7%)

### 10 Logistic Regression
Working thru the Chapter 10 in the supplemental R for Statistical Learning
```{r}
default_trn
```
```{r}
# default status into 0,1 for linear regression. 
default_trn_lm <- default_trn
default_trn_lm %>% default_trn_lm %>% 
                   mutate(default = if_else(default == "No,", 0), 1)  
```

