---
title: "Ch3_Linear_Regression"
author: "G Smith"
date: "September 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 3 Linear Regression

My notes and worked examples for linear regression.

```{r}
# Loading my standard work environment libraries and the ISLR library
library(broom)
library(gridExtra)
library(GGally)
library(knitr)
library(ISLR)
library(car)
library(tidyverse)
```
## 3.6 Lab: Linear Regression

3.6.1 Libraries

```{r}
# ISLR is already laoded
# Loading MASS library
# Note that the MASS library may mess-up the dplry select() function
# I'm also practicing using tibbies, ggplot and the broom packages
library(MASS)
library(dplyr)
# this was a recommended solution; define select to be the dplyr select() function
select <- dplyr::select
rename <- dplyr::rename
```

```{r}
# loading Boston data set
data(Boston)
head(Boston)
attach(Boston)
```
## 3.6.2 Simple Linear Regression

```{r}
# creating first LM and playing w/ making it tidy
lm.fit <- Boston %>% 
  lm(medv~lstat, data = .)
# looking at the broom functions
lm.fit.tidy <- tidy(lm.fit, conf.int = TRUE, conf.level = .95)
lm.fit.tidy
```
```{r}
nrow(lm.fit.tidy)
```


```{r}
names(lm.fit.tidy)
```

```{r}
lm.fit.aug <- augment(lm.fit)
lm.fit.aug
```

```{r}
lm.fit.gla <- glance(lm.fit)
lm.fit.gla
```
```{r}
# using the predict() function
# first build a vector of predictor values
lstat.p <- c(1:30)
# run predict() for p_hat and confidence intervals
# the data.frame() is importatant
lm.fit.p_hat <- predict(lm.fit, data.frame(lstat = lstat.p), interval = "confidence")
lm.fit.p_hat
# tidy p_hat and lstat.p by making them into tibbles
lstat.p <- as.tibble(lstat.p)
lm.fit.p_hat <- as.tibble(lm.fit.p_hat)
# place the predictor values in the p_hat tibble and rename value as lstat
lm.fit.p_hat <- bind_cols(lstat.p,lm.fit.p_hat)
lm.fit.p_hat <- lm.fit.p_hat %>% 
  rename(lstat = value, medv = fit)
lm.fit.p_hat
```

```{r}
# plotting the fitted value for medv vs lstat with lower and upper conf intervals
lm.fit.p_hat %>% 
  ggplot(aes(lstat, medv, ymin = lwr, ymax = upr)) +
  geom_errorbar()
```
```{r}
# and now plotting with geom_smooth
lm.fit.p_hat %>% 
  ggplot(aes(lstat, medv, ymin = lwr, ymax = upr)) +
  geom_smooth(method = lm) + 
  geom_errorbar()
```
```{r}
# using plot() and abline() om the lm.fit model
# produces 4 diagnostic plots
par(mfrow=c(2,2))
plot(lm.fit)
```
```{r}
par(mfrow=c(1,1))
plot(hatvalues(lm.fit))
```

```{r}
# using ggplot isn't as easy as using the basic abline(lm.fit) call
# some more plottiong using ggplot
# need to get the intercept and slope from lm.fit.tidy
intercept = pull(lm.fit.tidy[1,2])
slope = pull(lm.fit.tidy[2,2])
# plot lstat vs medv and add abline
Boston %>% 
  ggplot(aes(lstat,medv)) +
  geom_point(alpha = 0.5, color = "red") +
  geom_abline(aes(intercept = intercept, slope = slope))
```
```{r}
# plot(predict(lm.fit), residuals(lm.fit)) is more direct
# than using ggplot
pred <- as.tibble(predict(lm.fit))
pred <- pred %>% 
  rename(pred = value)
resid <- as.tibble(residuals(lm.fit))
resid <-  resid %>%
  rename(resid = value)
tab <- bind_cols(pred,resid)
tab %>% 
ggplot(aes(pred, resid)) + 
  geom_point(alpha = 0.5)
```
```{r}
# using the broom augmented results lm.fit.aug
# this is a lot easier than the above!
lm.fit.aug %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "red", se = FALSE) +
  geom_hline(aes(yintercept = 0))
```
looks like there's some non-linearity. 
```{r}
# looking for outliers using studentized reiduals
lm.fit.aug
lm.fit.aug %>% 
  ggplot(aes(.fitted, .resid/.sigma)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
```
Note several potential outliers above +3. But that's probably because the residuals show a clear pattern and the model isn't an especially good fit

```{r}
# looking for high-leverage points
# plotting .hat (leverage) values and determining the observation w/ the largest leverage
# 
max_leverage <- lm.fit.aug %>% filter(.hat == max(.hat))
max_leverage
max_cooks <- lm.fit.aug %>% filter(.cooksd == max(.cooksd))
max_cooks
lm.fit.aug %>%
  ggplot(aes(.hat, .resid/.sigma)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "grey") +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") 
```
```{r}
# Cooks Distance - another way of looking for high leverage points
# chisq() is used as kind of a cut-off for IDing potential problem points
# nrow() gets the p+1 from the model
lm.fit.aug %>%
  ggplot(aes(x = 1:nrow(lm.fit.aug), y = .cooksd)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = qchisq(0.1,nrow(lm.fit.tidy))/2, color = "red") 
  
```

### 3.6.3 Multiple Linear Regression
```{r}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```
```{r}
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```
```{r}
tidy(lm.fit, conf.int = TRUE, conf.level = .95)
```
indus and age have high p.values and conf.int contains 0. Indicates that can't reject Ho: B(indus) and B(age) = 0, consider removing from model
```{r}
glance(lm.fit)
```
stron r.squared and F-stat = 108.08, very low p.value
```{r}
vif(lm.fit)
vif(lm.fit) >= 5
```
VIF for rad and tax above 5. high multi-collinearity. consider removing from model
```{r}
# basic diagnositc plots
par(mfrow=c(2,2))
plot(lm.fit)
```
```{r}
# updating the model by removing indus, age & rad
lm.fit.1 <- update(lm.fit, ~. - indus - age - rad)
summary(lm.fit.1)
```
```{r}
par(mfrow = c(2,2))
plot(lm.fit.1)
```
```{r}
vif(lm.fit.1) > 5
```
```{r}
anova(lm.fit, lm.fit.1)
```

### 3.6.4 Interaction Terms

```{r}
# these are the same models
# lm(medv ~ lstat + age + lstat:age, data = Boston) is medv ~ lstat + age + lstat x age
# medv ~ lstat * age is short-hand for medv ~ lstat + age + lstat x age
 lm.fit.2 <-  lm(medv ~ lstat * age, data = Boston)
 summary(lm.fit.2)
```
```{r}
par(mfrow = c(2,2))
plot(lm.fit.2)
```
note that the Cooks Distance for observation is above 1. 
```{r}
vif(lm.fit.2)
```
VIF is high for lstat. Expect a high VIF for an interaction term

### 3.6.5 Non-linear Transformations of the Predictors
```{r}
lm.fit.3 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit.3)
```
```{r}
par(mfrow = c(2,2))
plot(lm.fit.3)
```
```{r}
# higher order polynomials using poly(); in this case a 7th order polynomial
lm.fit.4 <-  lm(medv ~ poly(lstat, 7))
summary(lm.fit.4)
```
Note that the 6th and 7th poly aren't significant. Also, high order poly is probably overfitting
```{r}
par(mfrow = c(2,2))
plot(lm.fit.4)
```

## My Special Section on Bias-Variance Trade-off
Code is from a STAT course at U of IL/ Chapain-Urbana
illustrate the bias-variance tradeoff through simulation by fitting models of various complexity to the "true" (but assumed unknown) function f(x): Y = x^2
with irreducable error eps ~ N(0,.56), note sd = .75
```{r}
# define the true function f(x) = x^2
f <- function(x) {
        x^2
}
# generate sample data using the function below. The get_sim_data function calls f() above 
get_sim_data = function(f, sample_size = 100) {
  x <- runif(n = sample_size, min = 0, max = 1)
  eps <-  rnorm(n = sample_size, mean = 0, sd = 0.75)
  y <-  f(x) + eps
  data.frame(x, y)
}
# get the sim data
set.seed(1)
sim_data = get_sim_data(f)
sim_data
# define and run 4 model of increasing complexity
fit_0 <- lm(y ~ 1,                   data = sim_data)
fit_1 <-  lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 <-  lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 <-  lm(y ~ poly(x, degree = 9), data = sim_data)
# binding the sim and models into a tibble
fit_out <- as.tibble(bind_cols(x = sim_data$x, y = sim_data$y, true = (sim_data$x)^2, y0 = fit_0$fitted.values, y1 = fit_1$fitted.values, y2 = fit_2$fitted.values, y9 = fit_9$fitted.values))
```
```{r}
# plotting the sim data and model fits
fit_out %>% 
  ggplot() +
  geom_point(aes(x,y), alpha = 0.5) + 
    geom_line(aes(x,true), color = "black") +
    geom_line(aes(x,y0), color = "red") +
    geom_line(aes(x,y1), color = "green") +
    geom_line(aes(x,y2), color = "blue", linetype = "dashed") +
    geom_line(aes(x,y9), color = "darkgreen", linetype = "dashed")
```
```{r}
# fixing a set x = .9 and running 500 simulations for each of the 5 models to estimate bias, variance and MSE for each model at x = .9
set.seed(1)
n_sims <- 500
n_models <- 5
x <- tibble(x = .9)
# blank matrix to hold results
predictions <- matrix(0, nrow = n_sims, ncol = n_models)
# # run the simulations
for(sim in 1:n_sims) {
  sim_data <- get_sim_data(f) # generating a new data set for each simulation
# fitting each model
  fit_0 <- lm(y ~ 1,                   data = sim_data)
  fit_1 <- lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 <- lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_3 <- lm(y ~ poly(x, degree = 3), data = sim_data)
  fit_9 <-  lm(y ~ poly(x, degree = 9), data = sim_data)
# getting each model's predictions for each simulation
  predictions[sim,1] <- predict(fit_0, x)
  predictions[sim,2] <- predict(fit_1, x)
  predictions[sim,3] <- predict(fit_2, x)
  predictions[sim,4] <- predict(fit_3, x)
  predictions[sim,5] <- predict(fit_9, x)
}
fit.out.2 <- as.tibble(predictions)
fit.out.2 <- fit.out.2 %>% 
                rename(P0 = V1, P1 = V2, P2 = V3, P3 = V4, P9 = V5)
fit.out.2
```
```{r}
# making a boxplot of the results
fit.out.2 %>%  
  ggplot() +
  geom_boxplot(aes(x = 0, y = P0, middle = mean(P0))) +
  geom_jitter(aes(x = 0, y = P0), width = 0.15, height = 0, color = "blue", alpha = 0.25) +
  geom_boxplot(aes(x = 1, y = P1, middle = mean(P1))) +
  geom_jitter(aes(x = 1, y = P1), width = 0.15, height = 0,color = "red", alpha = 0.25) +
  geom_boxplot(aes(x = 2, y = P2, middle = mean(P2))) +
  geom_jitter(aes(x = 2, y = P2), width = 0.15, height = 0,color = "green", alpha = 0.25)+
  geom_boxplot(aes(x = 3, y = P3, middle = mean(P9))) +
  geom_jitter(aes(x = 3, y = P3), width = 0.15, height = 0,color = "orange", alpha = 0.25) +
  geom_boxplot(aes(x = 4, y = P9, middle = mean(P9))) +
  geom_jitter(aes(x = 4, y = P9), width = 0.15, height = 0,color = "dark green", alpha = 0.25) +
  geom_hline(yintercept = .9^2) +
  scale_x_continuous(breaks = c(0,1,2,3,4), labels = c("0","1","2","3","9")) +
  labs(x = "Polynomial Degree", y = "Predictions")
```
Note that as model complexity increases that bias decreases (the model's mean is closer to the true f(x) = x^2), but variance (about the model's mean) increases. Specifically note that a 2nd degree polynomial visually has "best" combination of bias and variance.

```{r}
b_v_results <- fit.out.2 %>% 
  summarise_all(funs(bias_2 = (mean(.) - f(x = 0.90))^2, var = var(.)))
mse_table <- b_v_results %>% 
             mutate(P0_mse = P0_bias_2 + P0_var, P1_mse = P1_bias_2 + P1_var, P2_mse = P2_bias_2 + P2_var, P3_mse = P3_bias_2 + P3_var, P9_mse = P9_bias_2 + P9_var)

mse_table
mse_table %>% 
  select(ends_with("mse"))
```
```{r}
fit_0 <- glance(lm(y ~ 1,                   data = sim_data))
fit_1 <- glance(lm(y ~ poly(x, degree = 1), data = sim_data))
fit_2 <- glance(lm(y ~ poly(x, degree = 2), data = sim_data))
fit_3 <- glance(lm(y ~ poly(x, degree = 3), data = sim_data))
fit_9 <- glance(lm(y ~ poly(x, degree = 9), data = sim_data))

combined <- bind_rows(fit_0,fit_1,fit_2,fit_3,fit_9)
combined <- bind_cols(Poly = c("0","1","2","3","9"), combined)
combined
```
Based on the models' mse and the F-Stat on the simulated training sets, good model choices would be linear, quad, or cubic. R^2 will always increase (or at least not decrease) as predictors are added. Remember - we don't know the true underling function f(x) = x^2. And, the bias, var and mse will be estimated by repeated sampling from a training set.

## 3.1 Simple Linear Regression
Working thru the examples in the section

```{r}
# load the Advertising data

Advertising <- as.tibble(Advertising)
Advertising
```

```{r}
# some basic data exploration
# first a summary dropping X1 the index
Advertising %>% 
  select(-X1) %>% 
  summary()
```
```{r}
# next pairwise plots
Advertising %>% 
  select(-X1) %>% 
  ggpairs(progress = FALSE)
```
The plot seems to suggest pretty strong relationships between TV and sales, and radio and sales. No so much for newspaper and sales
```{r}
# lm for sales~TV
lm.fit.tv <- Advertising %>% 
                lm(sales~TV, data = .)
summary(lm.fit.tv)
```
```{r}
# tidy, augmnet and glance the model; get the intercept and slope for plotting
lm.fit.tv.tidy <- tidy(lm.fit.tv)
lm.fit.tv.aug <- augment(lm.fit.tv)
lm.fit.tv.aug
lm.fit.tv.gla <- glance(lm.fit.tv)
lm.fit.tv.tidy
intercept <- pull(lm.fit.tv.tidy[1,2])
intercept
slope <- pull(lm.fit.tv.tidy[2,2])
slope
```
```{r}
# plot of sales vs TV w/ regression line
Advertising %>% 
  ggplot(aes(TV, sales)) +
  geom_point(alpha = 0.5, color = "red") +
  geom_abline(aes(intercept = intercept, slope = slope,))
```
Looks like it droops at left end (lower TV value), and has a cone-like spread

```{r}
# an easier way to make the plot above is to use the stat_smooth() function
# it fits a lm (in this case) regression line to the data and adds the regression line w/ a 95% CI
# Here I'm using the augmented tibble
lm.fit.tv.aug %>% 
  ggplot(aes(TV, sales)) +
  geom_point(alpha = 0.5, color = "red") +
  stat_smooth(method = lm, level = 0.95)
```

```{r}
# plot fitted vs residuals
lm.fit.tv.aug %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "red", se = FALSE) +
  geom_hline(aes(yintercept = 0))
```
Looks linear. But the residuals are cone shaped: non-constant variance in error terms. could try lm(log(sales ~ TV))
```{r}
# plot of sales vs fitted
lm.fit.tv.aug %>% 
  ggplot(aes(sales, .fitted)) +
  geom_point(alpha = 0.5)
```
```{r}
# looking for high leverage points; plotting studentized residuals vs .hat (leverage); cutoff is |3| 
lm.fit.tv.aug %>% 
  ggplot(aes(.hat, .resid/.sigma)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
```
```{r}
# high leverage points w/ Cooks Distance
# chisq() is used as kind of a cut-off for IDing potential problem points
# nrow() gets the p+1 from the model
lm.fit.tv.aug %>%
  ggplot(aes(x = 1:nrow(lm.fit.tv.aug), y = .cooksd)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = qchisq(0.1,nrow(lm.fit.tv.tidy))/2, color = "red") 
```

No apparent high leverage points
```{r}
# calculating r2 by hand just for practice
tss <- sum((Advertising$sales - mean(Advertising$sales))^2)
tss
rss <- sum((lm.fit.tv.aug$sales - lm.fit.tv.aug$.fitted)^2)
rss
r2 <- 1-(rss/tss)
r2
```
## 3.2 Multiple Linear Regression

```{r}
# regressing sales on TV, radio, newspaper
lm.fit.mult <- Advertising %>% 
                lm(sales ~ TV + radio + newspaper, data = .)
# Collinearity test using VIF from "car" package
vif(lm.fit.mult)
sqrt(vif(lm.fit.mult)) > 2

summary(lm.fit.mult)
lm.fit.mult.tidy <- tidy(lm.fit.mult, conf.int = TRUE, conf.level = .95)
lm.fit.mult.aug <- augment(lm.fit.mult)
lm.fit.mult.gla <- glance(lm.fit.mult)
lm.fit.mult.tidy
```
```{r}
lm.fit.mult.aug
```

```{r}
lm.fit.mult.gla
```
```{r}
# practising calculating R^2 by hand
names(lm.fit.mult.aug)
tss <- sum((lm.fit.mult.aug$sales - mean(lm.fit.mult.aug$sales))^2)
tss
rss <- sum((lm.fit.mult.aug$sales - lm.fit.mult.aug$.fitted)^2)
rss
r2 <- 1-(rss/tss)
r2
```
## 3.3.1 Qualitative Predictors
```{r}
# loading the Credit data set
data(Credit)
# making the Credit data set a tibble
Credit <- as.tibble(Credit)
Credit
```
```{r}
# running a basic summary of the data
Credit %>% 
  summary()
```
```{r}
# next pairwise plots and correlation
Credit %>% 
  select(-ID) %>% 
  ggpairs(progress = FALSE)
```
```{r}
# a linear regression w/ a qualitative variable w/ 2 levels
lm.fit.gen <- Credit %>% 
  lm(Balance ~ Gender, data =.)
lm.fit.gen.tidy <- tidy(lm.fit.gen, conf.int = TRUE, conf.level = 0.95)
lm.fit.gen.tidy
```
#### There doesn't seem to be a strong relationship between gender and credit card balance. t-stat = 0.4285 w/ p.value = .6685; can't reject the null Ho: B1 = 0
```{r}
# linear regression w/ a qualitative variable w/ 3 levels
# the lm() automatically generates the dummy variables, so no need to create them
lm.fit.eth <- Credit %>% 
  lm(Balance ~ Ethnicity, data = .)
lm.fit.eth.tidy <- tidy(lm.fit.eth, conf.int = TRUE, conf.level = 0.95)
lm.fit.eth.tidy
```
```{r}
# there doesn't appear to be a strong relationship between enthnicity and balace
# using glance() to check the model stats
lm.fit.eth %>% 
  glance()
```
#### very low F-stat = 0.043 and very high p.value. Cannot reject the null hypothesis Ho: B1 = B2 = 0

## 3.3.2 Extensions of the Linear Model

# Removing the Additive Assumption - Interactions
```{r}
# building a model that has an interaction
lm.fit.inter <- Advertising %>% 
                lm(sales ~ TV + radio + TV*radio, data =.)
lm.fit.inter.tidy <- tidy(lm.fit.inter, conf.int = TRUE, conf.level = TRUE)
lm.fit.inter.tidy
```
```{r}
lm.fit.inter.aug <- augment(lm.fit.inter)
lm.fit.inter.aug
```
```{r}
# checking the F-Stat of the model
glance(lm.fit.inter)
```
strong R^2 and very high F-stat. Can reject (fail to accept) the null Ho: B1 = B2 = B3 = 0
```{r}
# a plot of fitted vs residuals
lm.fit.inter.aug %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "red", se = FALSE) +
  geom_hline(aes(yintercept = 0))
```
a little droopy at the left end; suggest some, but not really bad, non-linearity
```{r}
# looking for high leverage points
lm.fit.inter.aug %>% 
  ggplot(aes(.hat, .resid/.sigma)) +
  geom_point(alpha = 0.5)+
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
```
looks like two high leverage points outside |3| lets find them 
`# high leverage points w/ Cooks Distance
```{r}
# chisq() is used as kind of a cut-off for IDing potential problem points
# nrow() gets the p+1 from the model
lm.fit.inter.aug            
lm.fit.inter.aug %>%
  ggplot(aes(x = 1:nrow(lm.fit.inter.aug), y = .cooksd)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = qf(0.5,nrow(lm.fit.inter.tidy),nrow(lm.fit.inter.aug) - nrow(lm.fit.inter.tidy)), color = "red")
```
Cooks' Distance shows one borderline high leverage point 

```{r}
lm.fit.inter.aug %>% 
  filter(.resid/.sigma < -3)
```
```{r}
# removing the high leverage points (any outside |3|) and rerunning the model
lm.fit.inter.2 <- lm.fit.inter.aug %>% 
  filter(.resid/.sigma <=3 & .resid/.sigma >= -3) %>% 
  lm(sales ~ TV + radio + TV*radio, data = .)
glance(lm.fit.inter.2)
```

```{r}
# looking for high leverage points
lm.fit.inter.2.aug <- augment(lm.fit.inter.2)
lm.fit.inter.2.aug %>% 
  ggplot(aes(.hat, .resid/.sigma)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
  
```
So, it seems that there are NEW "high leverage"" points. 
```{r}
lm.fit.inter.3 <- lm.fit.inter.2.aug %>% 
  filter(.resid/.sigma <=3 & .resid/.sigma >= -3) %>% 
  lm(sales ~ TV + radio + TV*radio, data = .)
glance(lm.fit.inter.3)
```
```{r}
# looking for high leverage points
lm.fit.inter.3.aug <- augment(lm.fit.inter.3)
lm.fit.inter.3.aug %>% 
  ggplot(aes(.hat, .resid/.sigma)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
```

```{r}
# tidy the model results
tidy(lm.fit.inter.2, conf.int = TRUE, conf.level = 0.95)
```
not a big difference in the model results by removing the high leverage points
```{r}
# fitting a model w/ a quantitatve and qualitative variable
lm.fit.balance.2 <- Credit %>% 
                  lm(Balance ~ Income + Student, data = .)
tidy(lm.fit.balance.2, conf.int = TRUE, conf.level = 0.95)
```
```{r}
# fitting a model w/ a quantitative and qualitative variables and an inreaction
lm.fit.balance.3 <- Credit %>% 
                    lm(Balance ~ Income + Student + Income*Student, data = .)
tidy(lm.fit.balance.3, conf.int = TRUE, conf.level = 0.95)
```

### 3.3.3 Potential Problems

#### Non-linearity

An example using the Auto data set
```{r}
data(Auto)
Auto <- as.tibble(Auto)
Auto
```
```{r}
# fitting a lm() w/ a 1st degree poly
lm.fit.hp <- Auto %>% 
            lm(mpg ~ horsepower, data = .)
lm.fit.hp.tidy <- tidy(lm.fit.hp, conf.int = TRUE, conf.level = 0.95)
lm.fit.hp.tidy
```
```{r}
glance(lm.fit.hp)
```
High r.squared and low p.value can reject (fail to accept) Ho: B1 = 0
```{r}
# plotting fitted vs residuals
lm.fit.hp.aug <- augment(lm.fit.hp)
lm.fit.hp.aug %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "red", se = FALSE) +
  geom_hline(aes(yintercept = 0))
```
strong pattern in residuals suggests non-linearity

```{r}
# fitting a lm() with a 1st & 2nd degree poly. Note using the poly() causes problems w/ augment(). 
# need to use I()
lm.fit.hp2 <- Auto %>% 
              lm(log10(mpg) ~ horsepower + I(horsepower^2), data = .)
lm.fit.hp2.tidy <- tidy(lm.fit.hp2, conf.int = TRUE, conf.level = 0.95)
lm.fit.hp2.tidy
```
very low p.value for hp and hp^2
```{r}
glance(lm.fit.hp2)
```
r.squared has increase from .6059 w/ just the linear term for hp to .6876 w/ th inclusion of hp^2 term
very low p.value so reject (fail to accept) Ho: B1 = B2 = 0

#### Non-constant Variance of Error Terms

```{r}
# plotting the .fitted vs .resid for the quadradic model mpg ~ hp + hp^2
lm.fit.hp2.aug <- augment(lm.fit.hp2)
lm.fit.hp2.aug %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "red", se = FALSE) +
  geom_hline(aes(yintercept = 0)) +
  xlab("Fitted Log10 of MPG") +
  ggtitle("Log10 MPG ~ Horsepower + Horsepower^2")
```
very litte pattern to residuals by including hp and hp^2 in the model. Still looked like there was some non-constant variance of error terms
So, I re-did the model log10(mpg) ~ hp + hp^2

#### Correlation of Error Terms
```{r}
# here I'm checking for correlation of error terms by plottig .resid vs the observation .rownames
# I didn't expact a pattern/tracking to emerge since, for example, the observations were not from a time series.
# It was just practice
lm.fit.hp2.aug
lm.fit.hp2.aug %>% 
  ggplot(aes(x = .rownames, y = .resid)) +
  geom_point(alpha = 0.5)
```

#### Outliers - points y that are far from the value predicted by the model
Can check for outliers by a basic plot OR by plotting the "studentized residuals vs the fitted values. Consider removing observation outside |3|
```{r}
lm.fit.hp2.aug
lm.fit.hp2.aug %>%
  mutate(studres = .resid/.sigma) %>% 
  ggplot(aes(x = .fitted, y = studres)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red")
```
Looks like 2 outlier observations that should be considered for removed and the model rerun 

#### High Leverage Points - observations where the predictor variable has unusually low or high values compared compared to the others
```{r}
# Using Cood's D and F(0.5,p,n-p) as cutoff line. p is the number of model parameters including the intercept. n is the number of observations
n <- nrow(lm.fit.hp2.aug)
p <-  nrow(lm.fit.hp2.tidy)
lm.fit.hp2.aug %>%
  ggplot(aes(x = 1:nrow(lm.fit.hp2.aug), .cooksd)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = qf(0.5,p,n - p), color = "red")
```

Plot of Cooks' Distance doesn't show any high leverage points

#### Collinearity of Predictors - two or more predictor valiables are closely related to one another.

High levels of collinearity results in a decline in the t-statistic. as a result, may we (accept) fail to reject Ho: Bj = 0 when we should. Therefore the "power" (the probability of detecting a "non-zero" coefficient is reduced by collinearity! 
Variance Inflation Factor (VIF) can be used to assess "multi-collinearity" between 2 or more variables. Min VIF = 1, cutoff is usually a VIF 5 or above
```{r}
# Evaluate Multi-collinearity - 
Credit
lm.fit.mod1 <- Credit %>% 
               lm(Balance ~ Age + Limit, data = .)
summary(lm.fit.mod1)
```
High t-stats and low p.value for all coefficients. Fail to accept (that is reject) Ho: B1 = B2 = 0 
```{r}
vif(lm.fit.mod1) # variance inflation factors 
```
 VIF indicates no problem w/ multi-colinearity 
```{r}
# including "Rating" in the model; known to have a high correlaion w/ "Limit". Also "Student"
 lm.fit.mod2 <- Credit %>% 
              lm(Balance ~ Age + Limit + Rating + Student, data = .)
summary(lm.fit.mod2)
```
High collinearity between Limit and Rating results in low t-stats for both. Would fail to reject (that is accept) Ho: B1 = B2 = 0

```{r}
# Checking VIF
vif(lm.fit.mod2)
```
Very high VIFs (160+) for Limit and Age. Should remove one from the model
