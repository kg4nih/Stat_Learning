---
title: "Ch5_Resampling"
author: "G Smith"
date: "November 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 5 Resampling Methods

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r echo=FALSE, warning=FALSE}
# Loading my standard work environment libraries and the ISLR library
library(MASS) # LDA package
library(class) # K-Nearest Neighbors package
library(broom)
library(gridExtra)
library(GGally)
library(knitr)
library(ISLR)
library(boot)
library(car)
library(caret)
library(e1071)
library(tidyverse)
# note that dplyr select() is masked by MASS library. Need to use dplyr::select()
# my own function for calculating classification error
calc_class_err <- function(actual, predicted) {
  mean(actual != predicted)
}
```
XXX 5.3 Lab: Cross-Validation and the Bootstrap
Working thru the lab examples
#### 5.3.1 Validation Set Approach
```{r}
# libraries are already loaded
set.seed(1)
Auto <- as.tibble(Auto)
Auto
train = sample(392,196)
lm.fit <- Auto %>% 
          lm(mpg ~ horsepower, data = ., subset = train)
summary(lm.fit)
```
```{r}
lm.pred <- predict(lm.fit, newdata = Auto[-train,]) # running predict on the test/validation set
mean((Auto$mpg[-train] - lm.pred)^2) # calculating MSE
```

```{r}
# fitting polynomials 1 thru 10 
val.error = rep(0,10) # vector of 0s to hold MSE results
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto, subset = train)
  glm.pred <- predict(glm.fit, newdata = Auto[-train,])
  val.error[i] <-  mean((Auto$mpg[-train] - glm.pred)^2)
}
```
```{r}
val.error # minimum MSE is for poly = 6, but it's not much different than poly = 2; go w/ the simpler model
plot(val.error, xlab = "Degreee of Poly", ylab = "MSE")
```
#### 5.3.2 LOOCV
```{r}
# same as Validation set approach - using the same training set and fitting 1 - 10 polys
Auto.trn <- Auto[train,]
cv.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn)
  cv.error[i] <- cv.glm(Auto.trn, glm.fit)$delta[1] # cv.glm does cross-validation; the default is LOOCV
}
```
```{r}
cv.error # minimum MSE is for poly = 7, but there's not a big difference between poly = 2 and poly = 7. So, go w/ the simpler model
plot(cv.error, xlab = "Degreee of Poly", ylab = "MSE")
```
```{r}
# Let's check the MSE for poly 1 - 10 models on the test data
test.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn) # fitting the models on the training data
  glm.pred <- predict(glm.fit, newdata = Auto[-train,]) # using the test data for predictions 
  test.error[i] <-  mean((Auto$mpg[-train] - glm.pred)^2) # checking the MSE for the models
}
test.error
```
test MSE is lowest for poly = 2 and poly = 6; 

#### 5.3.3 k_Fold Cross-Validation
```{r}
kcv.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn)
  kcv.error[i] <- cv.glm(Auto.trn, glm.fit, K = 10)$delta[1] # cv.glm does cross-validation; setting K = 10 for 10 fold cross-validation
}
kcv.error # minimum MSE is for poly = 6, but there's not a big difference between poly = 2 and poly = 6. So, go w/ the simpler model
plot(kcv.error, xlab = "Degreee of Poly", ylab = "MSE")
```

### 5.4 Exercises - Applied
#### 5 - Estimating Test Error
##### 5a - Fit logistic model
```{r}
set.seed(1)
Default <- as.tibble(Default)
Default
log.fit <- Default %>% 
           glm(default ~ income + balance, family = binomial, data = .)
summary(log.fit)
```
##### 5b - Validation Approach 
```{r}
trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
default.trn <- Default[trn.index,]
dim(default.trn)
default.tst <- Default[-trn.index,]
dim(default.tst)
```
```{r}
log_fit <- default.trn %>% 
            train(default ~ income + balance, method = "glm", family = binomial, data = .)
summary(log_fit)
```
```{r}
log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
log_tab <- table(predicted = log_pred, actual = default.tst$default)
log_conf <- confusionMatrix(log_tab, positive = "Yes")
log_conf
```
```{r}
calc_class_err(predicted = log_pred, actual = default.tst$default)
```
```{r}
val.error <- rep(0,100) # intializing vector to hold misclassification errpr value
for( i in 1:100) {
  set.seed(i) # uses a new seed for each training/test set split and builds training & test set
  trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
  default.trn <- Default[trn.index,]
  default.tst <- Default[-trn.index,]
# builts log model
  log_fit <- default.trn %>% 
            train(default ~ income + balance, method = "glm", family = binomial, data = .)
# builds prediction
  log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
# captures misclassification error
  val.error[i] <- calc_class_err(predicted = log_pred, actual = default.tst$default)
}
```
```{r}
val.error <- as.tibble(val.error)
val.error
ggplot(val.error) +
  geom_density(aes(x=value))

```
```{r}
mean(val.error$value)
```
```{r}
sd(val.error$value)
```
```{r}
val.error2 <- rep(0,100) # intializing vector to hold misclassification errpr value
for( i in 1:100) {
  set.seed(i) # uses a new seed for each training/test set split and builds training & test set
  trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
  default.trn <- Default[trn.index,]
  default.tst <- Default[-trn.index,]
# builts log model
  log_fit <- default.trn %>% 
            train(default ~ income + balance + student, method = "glm", family = binomial, data = .)
# builds prediction
  log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
# captures misclassification error
  val.error2[i] <- calc_class_err(predicted = log_pred, actual = default.tst$default)
}
val.error2 <- as.tibble(val.error2)
val.error2
 
ggplot(NULL) +
  geom_density(aes(x = val.error2$value), color = "red") +
  geom_density(aes(x = val.error$value), color = "blue") +
  xlab("Estimated Test Error") +
  ggtitle("Estimating Test Error using the Validation Approach") +
  annotate("text", x = -Inf, y = Inf, hjust = -.2, vjust = 2, label = "default ~ income + balance", color = "blue") +
  annotate("text", x = -Inf, y = Inf, hjust = -.1, vjust = 4, label = "default ~ income + balance + student", color = "red") +
  geom_vline(aes(xintercept = mean(val.error$value)), color = "blue") +
  geom_vline(aes(xintercept = mean(val.error2$value)), color = "red")
```
Estimated test error is higher when the model includes student status
In the following sections I'm just playing around to see if the differences in means are statistically. 
```{r}
B <- 10000
N <- 50
Xhat_1 <- replicate(B, {
  X <- sample_n(val.error, size=N, replace = TRUE)
  mean(X$value)
})
Xhat_1 <- as.tibble(Xhat_1)
mean(Xhat_1$value)
sd(Xhat_1$value)
```
```{r}
ggplot(data = Xhat_1) +
  geom_histogram(aes(x = value), bins = 20)
```
```{r}
ggplot(data = Xhat_1) +
  geom_qq_line(aes(sample = value)) +
  geom_qq(aes(sample = value))
```
```{r}
B <- 10000
N <- 50
Xhat_2 <- replicate(B, {
  X <- sample_n(val.error2, size=N, replace = TRUE)
  mean(X$value)
})
Xhat_2 <- as.tibble(Xhat_2)
mean(Xhat_2$value)
sd(Xhat_2$value)
```
```{r}
Xhat_1_CI <- c(mean(Xhat_1$value) - qnorm(.975)*sd(Xhat_1$value),mean(Xhat_1$value) + qnorm(.975)*sd(Xhat_1$value) )
Xhat_1_CI
```
#### 6 Bootstrap to Standard Error Estimates
##### 6a 
```{r}
set.seed(1)
# using the training set
log.fit <- default.trn %>% 
           train(default ~ income + balance, method = "glm", family = binomial, data = .)
summary(log.fit)
```
##### 6b Using Bootstrap to estimate the SE of the predictor coefficients 
```{r}
# building my own boot.fn() per the text book
boot.fn <- function(data,index) {
  return(coef(glm(default ~ income + balance, data = data, subset = index, family = binomial)))
}
boot.fn(Default, 1:100)
```
```{r}
boot(default.trn,boot.fn,1000)
```
#### 7 LOOCV
##### 7a
```{r}
Weekly # looking at the tibble
log.fit <- Weekly %>% # fitting a logistic model to all the data
           train(Direction ~ Lag1 + Lag2, method = "glm", family = binomial, data = .)
summary(log.fit)
```
##### 7b
```{r}
log.fit <- Weekly[-1,] %>% # fitting a logistic model to all the data except the first observation
           train(Direction ~ Lag1 + Lag2, method = "glm", family = binomial, data = .)
summary(log.fit)
```
##### 7c
```{r}
# predicting first observation using the LOOCV model
  log.pred <- predict(log.fit, newdata = Weekly[1,], type = "raw")
log.pred
Weekly[1,]$Direction
# captures misclassification error
  calc_class_err(predicted = log.pred, actual = Weekly[1,]$Direction)
```
##### 7d & e
```{r}
loocv.error <- rep(0,nrow(Weekly)) # building a vector to capture classification errors for LOOCV
# a loop to do LOOCV and predicitons
for(i in 1:nrow(Weekly)) {
# builts log model leaving out the ith observation
  log.fit <- Weekly[-i,]%>% 
             train(Direction ~ Lag1 + Lag2, method = "glm", family = binomial, data = .)
# builds prediction for the ith observation
  log.pred <- predict(log.fit, newdata = Weekly[i,], type = "raw")
# captures misclassification error
  loocv.error[i] <- calc_class_err(predicted = log.pred, actual = Weekly[i,]$Direction)
}
mean(loocv.error)
```
```{r}
# doing k=10 k-fold CV tp compare to LOOCV
kfold.error <- rep(0,100) # going to fit the model to 10 training/test sets
# need a cost function since Direction is binary
cost <- function(Direction, c = 0) {
  mean(abs(Direction - c) > 0.5)}
for( i in 1:100) {
  set.seed(i) # uses a new seed for each training/test set split and builds training & test set
  trn.index <- createDataPartition(Weekly$Direction,p = 0.5, list = FALSE, times = 1)
  Weekly.trn <- Weekly[trn.index,]
  Weekly.tst <- Weekly[-trn.index,]
# builts log model
  log.fit <- glm(Direction ~ Lag1 + Lag2, family = binomial, data = Weekly.trn)
  summary(log.fit)
# 10 fold CV using cv.glm
  kfold.error[i] <- cv.glm(Weekly.trn, log.fit, cost, K = 10)$delta[1]
}
mean(kfold.error)

```
So, LOOCV and k-Fold about the same estimate for test error. But k-Fold is much quicked than LOOCV

#### 8
##### 8a
```{r}
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2+rnorm(100)
sim.data <- tibble(x,y)
sim.data
```
##### 8b
```{r}
sim.data %>% 
  ggplot(aes(x,y)) +
  geom_point()
```
##### 8c
```{r}
set.seed(1)
cv.error <- rep(0,4)
for (i in 1:4      ) {
  lm.fit <- glm(y ~ poly(x , i), data = sim.data)
  cv.error[i] <- cv.glm(sim.data, lm.fit)$delta[1]
}
cv.error
```
```{r}
set.seed(2)
cv.error <- rep(0,4)
for (i in 1:4) {
  lm.fit <- glm(y ~ poly(x , i), data = sim.data)
  cv.error[i] <- cv.glm(sim.data, lm.fit)$delta[1]
}
cv.error
```
```{r}
lm.fit.1 <- glm(y ~ x, data = sim.data)
summary(lm.fit.1)
```
```{r}
plot(predict(lm.fit.1), rstudent(lm.fit.1))
```
```{r}
lm.fit.2 <- glm(y ~ x + I(x^2), data = sim.data)
summary(lm.fit.2)
```
```{r}
plot(predict(lm.fit.2), rstudent(lm.fit.2))
```
```{r}
lm.fit.3 <- glm(y ~ poly(x,3), data = sim.data)
summary(lm.fit.3)
```
```{r}
plot(predict(lm.fit.3), rstudent(lm.fit.3))
```
#### 9 Bootstrap
##### 9a
```{r}
Boston <- as.tibble(Boston)
Boston
mu_hat <- mean(Boston$medv)
mu_hat
```
##### 9b
```{r}
se_hat <- sd(Boston$medv)/sqrt(nrow(Boston))
se_hat
```
# building my own boot.fn() per the text book
```{r}
boot.fn <- function(data,index) {
  return(sd(Boston[index,]$medv)/sqrt(sum(!is.na(Boston[index,]$medv))))
}
boot.fn(Boston,1:100)
boot(Boston,boot.fn,1000)
```
##### 9 c & d
```{r}
boot.fn.2 <- function(data,index) {
  return(mean(Boston[index,]$medv))
}
boot.fn.2(Boston,1:100)
b <- boot(Boston,boot.fn.2,1000)
boot.ci(b, conf = 0.95, type = "bca") # using boot.ci to build the 95% CI of the boot strap results
```

```{r}
t.test(Boston$medv)
```
bootstrap CI for the mean is very close to the t.test CI
##### 9e
```{r}
med_hat <- median(Boston$medv)
med_hat
```
##### 9f
```{r}
boot.fn.3 <- function(data,index) {
  return(median(Boston[index,]$medv))
}
boot.fn.3(Boston,1:100)
b <- boot(Boston,boot.fn.3,1000)
b
boot.ci(b, conf = 0.95, type = "bca") # using boot.ci to build the 95% CI of the boot strap results
```
##### 9g
```{r}
q10_hat <- quantile(Boston$medv, probs = 0.1)
q10_hat
```
##### 9h
```{r}
boot.fn.4 <- function(data,index) {
  return(quantile(Boston[index,]$medv, probs = 0.10))
}
boot.fn.4(Boston,1:100)
b <- boot(Boston,boot.fn.4,1000)
b
boot.ci(b, conf = 0.95, type = "bca") # using boot.ci to build the 95% CI of the boot strap results
```

