---
title: "Ch5_Resampling"
author: "G Smith"
date: "November 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 5 Resampling Methods

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r echo=FALSE, warning=FALSE}
# Loading my standard work environment libraries and the ISLR library
library(MASS) # LDA package
library(class) # K-Nearest Neighbors package
library(broom)
library(gridExtra)
library(GGally)
library(knitr)
library(ISLR)
library(boot)
library(car)
library(caret)
library(e1071)
library(tidyverse)
# note that dplyr select() is masked by MASS library. Need to use dplyr::select()
# my own function for calculating classification error
calc_class_err <- function(actual, predicted) {
  mean(actual != predicted)
}
```
XXX 5.3 Lab: Cross-Validation and the Bootstrap
Working thru the lab examples
#### 5.3.1 Validation Set Approach
```{r}
# libraries are already loaded
set.seed(1)
Auto <- as.tibble(Auto)
Auto
train = sample(392,196)
lm.fit <- Auto %>% 
          lm(mpg ~ horsepower, data = ., subset = train)
summary(lm.fit)
```
```{r}
lm.pred <- predict(lm.fit, newdata = Auto[-train,]) # running predict on the test/validation set
mean((Auto$mpg[-train] - lm.pred)^2) # calculating MSE
```

```{r}
# fitting polynomials 1 thru 10 
val.error = rep(0,10) # vector of 0s to hold MSE results
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto, subset = train)
  glm.pred <- predict(glm.fit, newdata = Auto[-train,])
  val.error[i] <-  mean((Auto$mpg[-train] - glm.pred)^2)
}
```
```{r}
val.error # minimum MSE is for poly = 6, but it's not much different than poly = 2; go w/ the simpler model
plot(val.error, xlab = "Degreee of Poly", ylab = "MSE")
```
#### 5.3.2 LOOCV
```{r}
# same as Validation set approach - using the same training set and fitting 1 - 10 polys
Auto.trn <- Auto[train,]
cv.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn)
  cv.error[i] <- cv.glm(Auto.trn, glm.fit)$delta[1] # cv.glm does cross-validation; the default is LOOCV
}
```
```{r}
cv.error # minimum MSE is for poly = 7, but there's not a big difference between poly = 2 and poly = 7. So, go w/ the simpler model
plot(cv.error, xlab = "Degreee of Poly", ylab = "MSE")
```
```{r}
# Let's check the MSE for poly 1 - 10 models on the test data
test.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn) # fitting the models on the training data
  glm.pred <- predict(glm.fit, newdata = Auto[-train,]) # using the test data for predictions 
  test.error[i] <-  mean((Auto$mpg[-train] - glm.pred)^2) # checking the MSE for the models
}
test.error
```
test MSE is lowest for poly = 2 and poly = 6; 

#### 5.3.3 k_Fold Cross-Validation
```{r}
kcv.error <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = Auto.trn)
  kcv.error[i] <- cv.glm(Auto.trn, glm.fit, K = 10)$delta[1] # cv.glm does cross-validation; setting K = 10 for 10 fold cross-validation
}
kcv.error # minimum MSE is for poly = 6, but there's not a big difference between poly = 2 and poly = 6. So, go w/ the simpler model
plot(kcv.error, xlab = "Degreee of Poly", ylab = "MSE")
```

### 5.4 Exercises - Applied
#### 5 - Estimating Test Error
##### 5a - Fit logistic model
```{r}
set.seed(1)
Default <- as.tibble(Default)
Default
log.fit <- Default %>% 
           glm(default ~ income + balance, family = binomial, data = .)
summary(log.fit)
```
##### 5b - Validation Approach 
```{r}
trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
default.trn <- Default[trn.index,]
dim(default.trn)
default.tst <- Default[-trn.index,]
dim(default.tst)
```
```{r}
log_fit <- default.trn %>% 
            train(default ~ income + balance, method = "glm", family = binomial, data = .)
summary(log_fit)
```
```{r}
log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
log_tab <- table(predicted = log_pred, actual = default.tst$default)
log_conf <- confusionMatrix(log_tab, positive = "Yes")
log_conf
```
```{r}
calc_class_err(predicted = log_pred, actual = default.tst$default)
```
```{r}
val.error <- rep(0,100) # intializing vector to hold misclassification errpr value
for( i in 1:100) {
  set.seed(i) # uses a new seed for each training/test set split and builds training & test set
  trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
  default.trn <- Default[trn.index,]
  default.tst <- Default[-trn.index,]
# builts log model
  log_fit <- default.trn %>% 
            train(default ~ income + balance, method = "glm", family = binomial, data = .)
# builds prediction
  log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
# captures misclassification error
  val.error[i] <- calc_class_err(predicted = log_pred, actual = default.tst$default)
}
```
```{r}
val.error <- as.tibble(val.error)
val.error
ggplot(val.error) +
  geom_density(aes(x=value))

```
```{r}
mean(val.error$value)
```
```{r}
sd(val.error$value)
```
```{r}
val.error2 <- rep(0,100) # intializing vector to hold misclassification errpr value
for( i in 1:100) {
  set.seed(i) # uses a new seed for each training/test set split and builds training & test set
  trn.index <- createDataPartition(Default$default,p = 0.5, list = FALSE, times = 1)
  default.trn <- Default[trn.index,]
  default.tst <- Default[-trn.index,]
# builts log model
  log_fit <- default.trn %>% 
            train(default ~ income + balance + student, method = "glm", family = binomial, data = .)
# builds prediction
  log_pred <- predict(log_fit, newdata = default.tst, type = "raw")
# captures misclassification error
  val.error2[i] <- calc_class_err(predicted = log_pred, actual = default.tst$default)
}
val.error2 <- as.tibble(val.error2)
val.error2
 
ggplot(NULL) +
  geom_density(aes(x = val.error2$value), color = "red") +
  geom_density(aes(x = val.error$value), color = "blue") +
  xlab("Estimated Test Error") +
  ggtitle("Estimating Test Error using the Validation Approach") +
  annotate("text", x = -Inf, y = Inf, hjust = -.2, vjust = 2, label = "default ~ income + balance", color = "blue") +
  annotate("text", x = -Inf, y = Inf, hjust = -.1, vjust = 4, label = "default ~ income + balance + student", color = "red") +
  geom_vline(aes(xintercept = mean(val.error$value)), color = "blue") +
  geom_vline(aes(xintercept = mean(val.error2$value)), color = "red")
```
Estimated test error is higher when the model includes student status
In the following sections I'm just playing around to see if the differences in means are statistically. 
```{r}
B <- 10000
N <- 50
Xhat_1 <- replicate(B, {
  X <- sample_n(val.error, size=N, replace = TRUE)
  mean(X$value)
})
Xhat_1 <- as.tibble(Xhat_1)
mean(Xhat_1$value)
sd(Xhat_1$value)
```
```{r}
ggplot(data = Xhat_1) +
  geom_histogram(aes(x = value), bins = 20)
```
```{r}
ggplot(data = Xhat_1) +
  geom_qq_line(aes(sample = value)) +
  geom_qq(aes(sample = value))
```
```{r}
B <- 10000
N <- 50
Xhat_2 <- replicate(B, {
  X <- sample_n(val.error2, size=N, replace = TRUE)
  mean(X$value)
})
Xhat_2 <- as.tibble(Xhat_2)
mean(Xhat_2$value)
sd(Xhat_2$value)
```
```{r}
Xhat_1_CI <- c(mean(Xhat_1$value) - qnorm(.975)*sd(Xhat_1$value),mean(Xhat_1$value) + qnorm(.975)*sd(Xhat_1$value) )
Xhat_1_CI
```
#### 6 Bootstrap to Standard Error Estimates
##### 6a 
```{r}
set.seed(1)
# using the training set
log.fit <- default.trn %>% 
           train(default ~ income + balance, method = "glm", family = binomial, data = .)
summary(log.fit)
```
##### 6b Using Bootstrap to estimate the SE of the predictor coefficients 
```{r}
# building my own boot.fn() per the text book
boot.fn <- function(data,index) {
  return(coef(glm(default ~ income + balance, data = data, subset = index, family = binomial)))
}
boot.fn(Default, 1:100)
```
```{r}
boot(default.trn,boot.fn,1000)
```


